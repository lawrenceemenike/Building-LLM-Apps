{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlqeJVmnShRswXTEDKZD5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrenceemenike/Building-LLM-Apps/blob/main/LLM_Ingest_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYJgDA0TPyrl"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "from typing import List, Tuple\n",
        "\n",
        "import langchain\n",
        "import wandb\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.runnable import Runnable\n",
        "from langchain.cache import SQLiteCache\n",
        "from langchain.embeddings.cache import CacheBacked\n",
        "from langchain.documents_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.llm_cache = SQLiteCache(database_path=\"langchain.db\")\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "Lxx2t_jHQcE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(data_dir: str) -> List[Document]:\n",
        "    \"\"\"Load documents from a directory.\n",
        "\n",
        "    Args:\n",
        "      data_dir (str): The directory containing the markdown files\n",
        "\n",
        "    Returns:\n",
        "      List{Document}: A list of documnets\n",
        "      \"\"\"\n",
        "\n",
        "      md_files = list(map(str, pathlib.Path(data_dir).glob(\"*.md\")))\n",
        "      documents = [\n",
        "          UnstructuredMarkdownLoader(file_path=file_path).load()[0]\n",
        "          for file_path in md_files\n",
        "      ]\n",
        "      return documents\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "cQUZhd6_Qlih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_documnets(\n",
        "    documents: List[Documnets], chunk_size: int = 500, chunk_overlap=0\n",
        "                    ) -> List[Document]:\n",
        "    \"\"\"Split documents into chunks.\n",
        "\n",
        "    Args:\n",
        "        Documnets (List[Document]): A list of documents to split into chunks\n",
        "        chunk_size (int, optional): The size of each chunk. Defaults to 500\n",
        "        chunk_overlap (int, optional): The number of tokens to overlap between chunks. Defaults to 0\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of chunked documents\n",
        "    \"\"\"\n",
        "    mardown_text_splitter = MarkdownTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    split_documents = markdown_text_splitter.split_documents(documents)\n",
        "    return split_documents\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PnSHVgQERItl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(\n",
        "    documents,\n",
        "    vector_store_path: str = \"./vector_store\",\n",
        ") -> Chroma:\n",
        "    \"\"\"Create a ChromaDB vector store from a list of documents\n",
        "\n",
        "    Args:\n",
        "        documents (_type_): A list of documents to add to the vector store\n",
        "        vector_store_path (str, optional): the path to the vectgor store. Defaults to \"./vector_store\".\n",
        "\n",
        "    Returns:\n",
        "        chroma: A chromaDB vector store containing the documents\n",
        "    \"\"\"\n",
        "\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n",
        "    embedding_function = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embedding_function,\n",
        "        persist_directory=vector_store_path,\n",
        "    )\n",
        "\n",
        "    vector_store.persist()\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "rZS66oLCSt28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_dataset(documents: List[Document], run: \"wandb.run\"):\n",
        "  \"\"\"Log a dataset to wandb\n",
        "\n",
        "  Args:\n",
        "      documents (List[Document]): A list of documents to log\n",
        "      run (wandb.run): The wandb run object\n",
        "  \"\"\"\n",
        "\n",
        "  document_artifact = wandb.Artifact(name=\"documentation_dataset\", type=\"dataset\")\n",
        "  with document_artifiact.new_file(\"documents.json\") as f:\n",
        "    for document in documents:\n",
        "      f.write(document.json() + \"\\n\")\n",
        "\n",
        "  run.log_artifact(document_artifact)"
      ],
      "metadata": {
        "id": "ZPHVUS58UAyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_index(vector_store_dir: str, run: \"wandb.run\"):\n",
        "  \"\"\"Log a vector store to wandb\n",
        "\n",
        "  Args:\n",
        "      vector_store_dir (str): The directory containing the vector store to log run (wandb.run): The wandb run to log the artifact to.\n",
        "  \"\"\"\n",
        "  index_artifact = wandb.Artifact(nam=\"vector_store\", type=\"search_index\")\n",
        "  index_artifact.add_dir(vector_store_dir)\n",
        "  run.log_artifact(index_artifact)"
      ],
      "metadata": {
        "id": "4cjJfb8YVHHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_prompt(prompt: dict, run: \"wandb.run\"):\n",
        "  \"\"\"Log a prompt to wandb\n",
        "\n",
        "  Args:\n",
        "      prompt (dict): The prompt to log\n",
        "      run (wandb.run): The wandb run to log the artifact to.\n",
        "      \"\"\"\n",
        "    prompt_artifact = wandb.Artifact(name=\"chat_prompt\", type=\"prompt\")\n",
        "    with prompt_artifact.enew_file(\"prompt.json\") as f:\n",
        "      f.write(json.dumps(prompt))\n",
        "    run.log_artifact(prompt_artifact)"
      ],
      "metadata": {
        "id": "mcTIq0nPVoBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_data(\n",
        "    docs_dir: str,\n",
        "    chunk_sizeL int,\n",
        "    chunk_overlap: int,\n",
        "    vector_store_path: str,\n",
        ") -> Tuple[List[Document], Chroma]:\n",
        "\"\"\"Ingest a directory of markdown files into a vector store.\n",
        "\n",
        "  Args:\n",
        "      docs_dir (str):\n",
        "      chunk_size (int):\n",
        "      chunk_overlap (int):\n",
        "      vector_store_path (str):\n",
        "\"\"\"\n",
        "  # load the documents\n",
        "  documents = load_documents(docs_dir)\n",
        "  # split the documents into chunks\n",
        "  split_documents = chunk_documnets(documents, chunk_size, chunk_overlap)\n",
        "  # Creae document embeddings and store them in a vector store\n",
        "  vector_store = create_vector_store(split_documents, vector_store_path)\n",
        "  return split_documents, vector_store"
      ],
      "metadata": {
        "id": "JhG_isr0WSan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pathlib\n",
        "\n",
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--docs_dir\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The directory containing the wandb documentation\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--chunk_size\",\n",
        "        type=int,\n",
        "        default=500,\n",
        "        help=\"The number of tokens to include in each document chunk\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--chunk_overlap\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"The number of tokens to overlap between document chunks\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--vector_store\",\n",
        "        type=str,\n",
        "        default=\"./vector_store\",\n",
        "        help=\"The directory to save or load the Chroma db to/from\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prompt_file\",\n",
        "        type=pathlib.Path,\n",
        "        default=\"./chat_prompt.json\",\n",
        "        help=\"The path to the chat prompt to use\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--wandb_project\",\n",
        "        default=\"llmapps\",\n",
        "        type=str,\n",
        "        help=\"The wandb project to use for storing artifacts\"\n",
        "    )\n",
        "\n",
        "    return parser"
      ],
      "metadata": {
        "id": "4TYF889sXwiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = get_parser()\n",
        "    args = parser.parse_args()\n",
        "    run = wandb.init(project=args.wandb_project, config=args)\n",
        "    documents, vector_store = ingest_data(\n",
        "        docs_dir=args.docs_dir,\n",
        "        chunk_size=args.chunk_size,\n",
        "        chunk_overlap=args.chunk_overlap,\n",
        "        vector_store_path=args.vector_store,\n",
        "    )\n",
        "    log_dataset(documents, run)\n",
        "    log_index(args.vector_store, run)\n",
        "    log_prompt(json.load(args.prompt_file.open(\"r\")), run)\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2sf2TPpEaaZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEd5ihMhXwq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}